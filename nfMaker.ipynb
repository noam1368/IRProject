{"cells":[{"cell_type":"markdown","id":"4aace43e","metadata":{"colab_type":"text","id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/noam1368/IRProject/blob/main/indexText.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","id":"3cb61452","metadata":{"id":"3cb61452"},"source":["<a href=\"https://colab.research.google.com/github/noam1368/IRProject/blob/main/indexTitle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","id":"a00e032c","metadata":{"id":"a00e032c"},"source":["***Important*** DO NOT CLEAR THE OUTPUT OF THIS NOTEBOOK AFTER EXECUTION!!!"]},{"cell_type":"code","execution_count":1,"id":"5ac36d3a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ac36d3a","nbgrader":{"grade":false,"grade_id":"cell-Worker_Count","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"a0ca7a50-2b2f-46b1-98fe-2ebc40b38c5c"},"outputs":[{"name":"stdout","output_type":"stream","text":["NAME          PLATFORM  WORKER_COUNT  PREEMPTIBLE_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n","cluster-0a65  GCE       3                                       RUNNING  us-central1-c\r\n"]}],"source":["# if the following command generates an error, you probably didn't enable \n","# the cluster security option \"Allow API access to all Google Cloud services\"\n","# under Manage Security â†’ Project Access when setting up the cluster\n","!gcloud dataproc clusters list --region us-central1"]},{"cell_type":"markdown","id":"51cf86c5","metadata":{"id":"51cf86c5"},"source":["# Imports & Setup"]},{"cell_type":"code","execution_count":2,"id":"bf199e6a","metadata":{"id":"bf199e6a","nbgrader":{"grade":false,"grade_id":"cell-Setup","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"381c187e-ffd3-4d8f-cda5-ed3bbb512472"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes"]},{"cell_type":"code","execution_count":3,"id":"7f-q_ZXOPObJ","metadata":{"id":"7f-q_ZXOPObJ","outputId":"09a8a438-1037-47a9-d9a6-9a9015a1c4d5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: nltk==3.7 in /opt/conda/miniconda3/lib/python3.8/site-packages (3.7)\n","Requirement already satisfied: joblib in /opt/conda/miniconda3/lib/python3.8/site-packages (from nltk==3.7) (1.2.0)\n","Requirement already satisfied: tqdm in /opt/conda/miniconda3/lib/python3.8/site-packages (from nltk==3.7) (4.64.1)\n","Requirement already satisfied: regex>=2021.8.3 in /opt/conda/miniconda3/lib/python3.8/site-packages (from nltk==3.7) (2022.10.31)\n","Requirement already satisfied: click in /opt/conda/miniconda3/lib/python3.8/site-packages (from nltk==3.7) (7.1.2)\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0mNote: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install nltk==3.7"]},{"cell_type":"code","execution_count":4,"id":"d8f56ecd","metadata":{"id":"d8f56ecd","nbgrader":{"grade":false,"grade_id":"cell-Imports","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["#remove mark\n","import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":5,"id":"38a897f2","metadata":{"id":"38a897f2","nbgrader":{"grade":false,"grade_id":"cell-jar","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"a385f065-bdc9-457b-dadb-4667a36a68d4"},"outputs":[{"name":"stdout","output_type":"stream","text":["-rw-r--r-- 1 root root 247882 Jan 13 12:44 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"]}],"source":["#remove mark\n","\n","# if nothing prints here you forgot to include the initialization script when starting the cluster\n","!ls -l /usr/lib/spark/jars/graph*"]},{"cell_type":"code","execution_count":6,"id":"47900073","metadata":{"id":"47900073","nbgrader":{"grade":false,"grade_id":"cell-pyspark-import","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["#remove mark\n","\n","from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *"]},{"cell_type":"code","execution_count":7,"id":"72bed56b","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":222},"id":"72bed56b","nbgrader":{"grade":false,"grade_id":"cell-spark-version","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"35e305e8-4eab-4fd2-9c7c-7a8c8d29ec22","scrolled":true},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://cluster-0a65-m.c.ass3-370307.internal:40331\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.1.3</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>yarn</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>PySparkShell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7fb04b28c520>"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["spark"]},{"cell_type":"code","execution_count":8,"id":"980e62a5","metadata":{"id":"980e62a5","nbgrader":{"grade":false,"grade_id":"cell-bucket_name","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# Put your bucket name below and make sure you can access it without an error\n","bucket_name = 'corpus_big' \n","full_path = f\"gs://{bucket_name}/\"\n","paths=[]\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","for b in blobs:\n","    if b.name != 'graphframes.sh': #todo change when we want to run on the original corpuse\n","        paths.append(full_path+b.name)"]},{"cell_type":"markdown","id":"cac891c2","metadata":{"id":"cac891c2"},"source":["***GCP setup is complete!*** If you got here without any errors you've earned 10 out of the 35 points of this part."]},{"cell_type":"markdown","id":"582c3f5e","metadata":{"id":"582c3f5e"},"source":["# Building an inverted index"]},{"cell_type":"markdown","id":"481f2044","metadata":{"id":"481f2044"},"source":["Here, we read the entire corpus to an rdd, directly from Google Storage Bucket and use your code from Colab to construct an inverted index."]},{"cell_type":"code","execution_count":9,"id":"e4c523e7","metadata":{"id":"e4c523e7","scrolled":false},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["parquetFile = spark.read.parquet(*paths)\n","doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd #this is with text becuase we doint the inverted index for the title in this file "]},{"cell_type":"markdown","id":"0d7e2971","metadata":{"id":"0d7e2971"},"source":["We will count the number of pages to make sure we are looking at the entire corpus. The number of pages should be more than 6M"]},{"cell_type":"code","execution_count":10,"id":"82881fbf","metadata":{"id":"82881fbf","outputId":"754b3eb1-8844-4fe9-82ca-09b11dda10c0"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["6348910"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# Count number of wiki pages\n","parquetFile.count()"]},{"cell_type":"markdown","id":"701811af","metadata":{"id":"701811af"},"source":["Let's import the inverted index module. Note that you need to use the staff-provided version called `inverted_index_gcp.py`, which contains helper functions to writing and reading the posting files similar to the Colab version, but with writing done to a Google Cloud Storage bucket."]},{"cell_type":"code","execution_count":11,"id":"121fe102","metadata":{"id":"121fe102","outputId":"327fe81b-80f4-4b3a-8894-e74720d92e35"},"outputs":[{"name":"stdout","output_type":"stream","text":["inverted_index_gcp.py\r\n"]}],"source":["# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n","%cd -q /home/dataproc\n","!ls inverted_index_gcp.py"]},{"cell_type":"code","execution_count":12,"id":"57c101a8","metadata":{"id":"57c101a8","outputId":"895bcfaa-5ecb-4437-c8df-afda967645cd","scrolled":true},"outputs":[],"source":["# adding our python module to the cluster\n","sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())"]},{"cell_type":"code","execution_count":13,"id":"c259c402","metadata":{"id":"c259c402"},"outputs":[],"source":["from inverted_index_gcp import InvertedIndex"]},{"cell_type":"markdown","id":"5540c727","metadata":{"id":"5540c727"},"source":["**YOUR TASK (10 POINTS)**: Use your implementation of `word_count`, `reduce_word_counts`, `calculate_df`, and `partition_postings_and_write` functions from Colab to build an inverted index for all of English Wikipedia in under 2 hours.\n","\n","A few notes: \n","1. The number of corpus stopwords below is a bit bigger than the colab version since we are working on the whole corpus and not just on one file.\n","2. You need to slightly modify your implementation of  `partition_postings_and_write` because the signature of `InvertedIndex.write_a_posting_list` has changed and now includes an additional argument called `bucket_name` for the target bucket. See the module for more details.\n","3. You are not allowed to change any of the code not coming from Colab. "]},{"cell_type":"code","execution_count":14,"id":"f3ad8fea","metadata":{"id":"f3ad8fea","nbgrader":{"grade":false,"grade_id":"cell-token2bucket","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"ename":"TypeError","evalue":"unsupported operand type(s) for /: 'str' and 'str'","output_type":"error","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)","Cell \u001B[0;32mIn[14], line 45\u001B[0m\n\u001B[1;32m     42\u001B[0m lens\u001B[38;5;241m=\u001B[39mdoc_text_pairs\u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m x:doc_length(x[\u001B[38;5;241m0\u001B[39m],x[\u001B[38;5;241m1\u001B[39m]))\n\u001B[1;32m     43\u001B[0m dict_len\u001B[38;5;241m=\u001B[39mlens\u001B[38;5;241m.\u001B[39mcollectAsMap()\n\u001B[0;32m---> 45\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnorm.pkl\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m     46\u001B[0m     pickle\u001B[38;5;241m.\u001B[39mdump(dict_len, f)\n\u001B[1;32m     48\u001B[0m index_src \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnf.pkl\u001B[39m\u001B[38;5;124m\"\u001B[39m\n","\u001B[0;31mTypeError\u001B[0m: unsupported operand type(s) for /: 'str' and 'str'"]}],"source":["RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","import math\n","english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords) #to remark\n","\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","def tokenize(text):\n","    \"\"\"\n","    This function aims in tokenize a text into a list of tokens. Moreover, it filter stopwords.\n","\n","    Parameters:\n","    -----------\n","    text: string , represting the text to tokenize.\n","\n","    Returns:\n","    -----------\n","    list of tokens (e.g., list of tokens).\n","    \"\"\"\n","    list_of_tokens = [token.group() for token in RE_WORD.finditer(text.lower()) if\n","                      token.group() not in all_stopwords]\n","    return list_of_tokens\n","\n","\n","def doc_length(text,id1):\n","    tokens = tokenize(text)\n","    d={}\n","    s=0\n","    for token in tokens:\n","        d[token]=d.get(token,0)+1\n","    for k,v in d.items():\n","        s+=v*v\n","    q=math.sqrt(s)\n","    return id1,q\n","\n","lens=doc_text_pairs.map(lambda x:doc_length(x[0],x[1]))\n","dict_len=lens.collectAsMap()\n","\n"]},{"cell_type":"code","execution_count":15,"id":"35a8f36b","metadata":{},"outputs":[{"data":{"text/plain":["6348910"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["len(dict_len)"]},{"cell_type":"code","execution_count":16,"id":"c9ecbed6","metadata":{"id":"c9ecbed6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying file://nf.pkl [Content-Type=application/octet-stream]...\n","\\ [1 files][ 84.7 MiB/ 84.7 MiB]                                                \n","Operation completed over 1 objects/84.7 MiB.                                     \n"]}],"source":["with open(f'nf.pkl', 'wb') as f:\n","    pickle.dump(dict_len, f)\n","\n","index_src = \"nf.pkl\"\n","index_dst = f'gs://new_index_bucket/{index_src}'\n","!gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":null,"id":"16eb06ec","metadata":{},"outputs":[],"source":[]}],"metadata":{"celltoolbar":"Create Assignment","colab":{"include_colab_link":true,"provenance":[]},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"}},"nbformat":4,"nbformat_minor":5}